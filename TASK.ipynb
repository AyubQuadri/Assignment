{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "provenance": [],
      "gpuType": "T4",
      "authorship_tag": "ABX9TyMfYP70ZZCZKSexycaG37+z",
      "include_colab_link": true
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    },
    "accelerator": "GPU"
  },
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/AyubQuadri/Assignment/blob/main/TASK.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 14,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "pQAZt4-SVSht",
        "outputId": "8cd4499c-3ce8-4281-9517-9df18a204f39",
        "collapsed": true
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Drive already mounted at /content/gdrive; to attempt to forcibly remount, call drive.mount(\"/content/gdrive\", force_remount=True).\n",
            "Train Epoch: 1 [0/59872] Loss: 2.325803\n",
            "Train Epoch: 1 [6400/59872] Loss: 0.271597\n",
            "Train Epoch: 1 [12800/59872] Loss: 0.179996\n",
            "Train Epoch: 1 [19200/59872] Loss: 0.385996\n",
            "Train Epoch: 1 [25600/59872] Loss: 0.364178\n",
            "Train Epoch: 1 [32000/59872] Loss: 0.025599\n",
            "Train Epoch: 1 [38400/59872] Loss: 0.307025\n",
            "Train Epoch: 1 [44800/59872] Loss: 0.044439\n",
            "Train Epoch: 1 [51200/59872] Loss: 0.104807\n",
            "Train Epoch: 1 [57600/59872] Loss: 0.079351\n",
            "\n",
            "Test set: Average loss: 0.0001, Accuracy: 9673/10000 (96.73%)\n",
            "\n",
            "Train Epoch: 2 [0/59872] Loss: 0.046477\n",
            "Train Epoch: 2 [6400/59872] Loss: 0.061661\n",
            "Train Epoch: 2 [12800/59872] Loss: 0.077074\n",
            "Train Epoch: 2 [19200/59872] Loss: 0.088976\n",
            "Train Epoch: 2 [25600/59872] Loss: 0.098698\n",
            "Train Epoch: 2 [32000/59872] Loss: 0.240937\n",
            "Train Epoch: 2 [38400/59872] Loss: 0.163877\n",
            "Train Epoch: 2 [44800/59872] Loss: 0.224243\n",
            "Train Epoch: 2 [51200/59872] Loss: 0.062104\n",
            "Train Epoch: 2 [57600/59872] Loss: 0.097684\n",
            "\n",
            "Test set: Average loss: 0.0001, Accuracy: 9718/10000 (97.18%)\n",
            "\n",
            "Train Epoch: 3 [0/59872] Loss: 0.024174\n",
            "Train Epoch: 3 [6400/59872] Loss: 0.073221\n",
            "Train Epoch: 3 [12800/59872] Loss: 0.042274\n",
            "Train Epoch: 3 [19200/59872] Loss: 0.113216\n",
            "Train Epoch: 3 [25600/59872] Loss: 0.008031\n",
            "Train Epoch: 3 [32000/59872] Loss: 0.009545\n",
            "Train Epoch: 3 [38400/59872] Loss: 0.013501\n",
            "Train Epoch: 3 [44800/59872] Loss: 0.037389\n",
            "Train Epoch: 3 [51200/59872] Loss: 0.061565\n",
            "Train Epoch: 3 [57600/59872] Loss: 0.015733\n",
            "\n",
            "Test set: Average loss: 0.0001, Accuracy: 9777/10000 (97.77%)\n",
            "\n",
            "Train Epoch: 4 [0/59872] Loss: 0.073921\n",
            "Train Epoch: 4 [6400/59872] Loss: 0.002211\n",
            "Train Epoch: 4 [12800/59872] Loss: 0.014090\n",
            "Train Epoch: 4 [19200/59872] Loss: 0.033470\n",
            "Train Epoch: 4 [25600/59872] Loss: 0.097024\n",
            "Train Epoch: 4 [32000/59872] Loss: 0.007482\n",
            "Train Epoch: 4 [38400/59872] Loss: 0.034504\n",
            "Train Epoch: 4 [44800/59872] Loss: 0.024024\n",
            "Train Epoch: 4 [51200/59872] Loss: 0.035882\n",
            "Train Epoch: 4 [57600/59872] Loss: 0.090230\n",
            "\n",
            "Test set: Average loss: 0.0001, Accuracy: 9735/10000 (97.35%)\n",
            "\n",
            "Train Epoch: 5 [0/59872] Loss: 0.032004\n",
            "Train Epoch: 5 [6400/59872] Loss: 0.125173\n",
            "Train Epoch: 5 [12800/59872] Loss: 0.022532\n",
            "Train Epoch: 5 [19200/59872] Loss: 0.008039\n",
            "Train Epoch: 5 [25600/59872] Loss: 0.076687\n",
            "Train Epoch: 5 [32000/59872] Loss: 0.018842\n",
            "Train Epoch: 5 [38400/59872] Loss: 0.051276\n",
            "Train Epoch: 5 [44800/59872] Loss: 0.004363\n",
            "Train Epoch: 5 [51200/59872] Loss: 0.040672\n",
            "Train Epoch: 5 [57600/59872] Loss: 0.012418\n",
            "\n",
            "Test set: Average loss: 0.0001, Accuracy: 9783/10000 (97.83%)\n",
            "\n",
            "Train Epoch: 6 [0/59872] Loss: 0.033502\n",
            "Train Epoch: 6 [6400/59872] Loss: 0.001155\n",
            "Train Epoch: 6 [12800/59872] Loss: 0.013228\n",
            "Train Epoch: 6 [19200/59872] Loss: 0.042933\n",
            "Train Epoch: 6 [25600/59872] Loss: 0.019690\n",
            "Train Epoch: 6 [32000/59872] Loss: 0.041347\n",
            "Train Epoch: 6 [38400/59872] Loss: 0.156452\n",
            "Train Epoch: 6 [44800/59872] Loss: 0.015938\n",
            "Train Epoch: 6 [51200/59872] Loss: 0.026838\n",
            "Train Epoch: 6 [57600/59872] Loss: 0.064861\n",
            "\n",
            "Test set: Average loss: 0.0001, Accuracy: 9759/10000 (97.59%)\n",
            "\n",
            "Train Epoch: 7 [0/59872] Loss: 0.034583\n",
            "Train Epoch: 7 [6400/59872] Loss: 0.000383\n",
            "Train Epoch: 7 [12800/59872] Loss: 0.004280\n",
            "Train Epoch: 7 [19200/59872] Loss: 0.000492\n",
            "Train Epoch: 7 [25600/59872] Loss: 0.011027\n",
            "Train Epoch: 7 [32000/59872] Loss: 0.001882\n",
            "Train Epoch: 7 [38400/59872] Loss: 0.059947\n",
            "Train Epoch: 7 [44800/59872] Loss: 0.022332\n",
            "Train Epoch: 7 [51200/59872] Loss: 0.049451\n",
            "Train Epoch: 7 [57600/59872] Loss: 0.002101\n",
            "\n",
            "Test set: Average loss: 0.0001, Accuracy: 9776/10000 (97.76%)\n",
            "\n",
            "Train Epoch: 8 [0/59872] Loss: 0.036892\n",
            "Train Epoch: 8 [6400/59872] Loss: 0.011042\n",
            "Train Epoch: 8 [12800/59872] Loss: 0.052079\n",
            "Train Epoch: 8 [19200/59872] Loss: 0.127846\n",
            "Train Epoch: 8 [25600/59872] Loss: 0.005906\n",
            "Train Epoch: 8 [32000/59872] Loss: 0.000865\n",
            "Train Epoch: 8 [38400/59872] Loss: 0.004539\n",
            "Train Epoch: 8 [44800/59872] Loss: 0.000841\n",
            "Train Epoch: 8 [51200/59872] Loss: 0.006818\n",
            "Train Epoch: 8 [57600/59872] Loss: 0.000204\n",
            "\n",
            "Test set: Average loss: 0.0001, Accuracy: 9801/10000 (98.01%)\n",
            "\n",
            "Train Epoch: 9 [0/59872] Loss: 0.000332\n",
            "Train Epoch: 9 [6400/59872] Loss: 0.000973\n",
            "Train Epoch: 9 [12800/59872] Loss: 0.000757\n",
            "Train Epoch: 9 [19200/59872] Loss: 0.003940\n",
            "Train Epoch: 9 [25600/59872] Loss: 0.010405\n",
            "Train Epoch: 9 [32000/59872] Loss: 0.000456\n",
            "Train Epoch: 9 [38400/59872] Loss: 0.001720\n",
            "Train Epoch: 9 [44800/59872] Loss: 0.005730\n",
            "Train Epoch: 9 [51200/59872] Loss: 0.034465\n",
            "Train Epoch: 9 [57600/59872] Loss: 0.027060\n",
            "\n",
            "Test set: Average loss: 0.0001, Accuracy: 9747/10000 (97.47%)\n",
            "\n",
            "Train Epoch: 10 [0/59872] Loss: 0.002343\n",
            "Train Epoch: 10 [6400/59872] Loss: 0.046939\n",
            "Train Epoch: 10 [12800/59872] Loss: 0.031831\n",
            "Train Epoch: 10 [19200/59872] Loss: 0.034753\n",
            "Train Epoch: 10 [25600/59872] Loss: 0.002393\n",
            "Train Epoch: 10 [32000/59872] Loss: 0.001841\n",
            "Train Epoch: 10 [38400/59872] Loss: 0.018791\n",
            "Train Epoch: 10 [44800/59872] Loss: 0.000237\n",
            "Train Epoch: 10 [51200/59872] Loss: 0.003271\n",
            "Train Epoch: 10 [57600/59872] Loss: 0.208681\n",
            "\n",
            "Test set: Average loss: 0.0001, Accuracy: 9816/10000 (98.16%)\n",
            "\n",
            "Train Epoch: 11 [0/59872] Loss: 0.000741\n",
            "Train Epoch: 11 [6400/59872] Loss: 0.003056\n",
            "Train Epoch: 11 [12800/59872] Loss: 0.000040\n",
            "Train Epoch: 11 [19200/59872] Loss: 0.001561\n",
            "Train Epoch: 11 [25600/59872] Loss: 0.009338\n",
            "Train Epoch: 11 [32000/59872] Loss: 0.038826\n",
            "Train Epoch: 11 [38400/59872] Loss: 0.002735\n",
            "Train Epoch: 11 [44800/59872] Loss: 0.001931\n",
            "Train Epoch: 11 [51200/59872] Loss: 0.046702\n",
            "Train Epoch: 11 [57600/59872] Loss: 0.005410\n",
            "\n",
            "Test set: Average loss: 0.0001, Accuracy: 9797/10000 (97.97%)\n",
            "\n",
            "Train Epoch: 12 [0/59872] Loss: 0.002787\n",
            "Train Epoch: 12 [6400/59872] Loss: 0.000214\n",
            "Train Epoch: 12 [12800/59872] Loss: 0.013393\n",
            "Train Epoch: 12 [19200/59872] Loss: 0.009290\n",
            "Train Epoch: 12 [25600/59872] Loss: 0.001625\n",
            "Train Epoch: 12 [32000/59872] Loss: 0.027619\n",
            "Train Epoch: 12 [38400/59872] Loss: 0.120708\n",
            "Train Epoch: 12 [44800/59872] Loss: 0.000332\n",
            "Train Epoch: 12 [51200/59872] Loss: 0.001441\n",
            "Train Epoch: 12 [57600/59872] Loss: 0.009480\n",
            "\n",
            "Test set: Average loss: 0.0001, Accuracy: 9814/10000 (98.14%)\n",
            "\n",
            "Train Epoch: 13 [0/59872] Loss: 0.084036\n",
            "Train Epoch: 13 [6400/59872] Loss: 0.011361\n",
            "Train Epoch: 13 [12800/59872] Loss: 0.000279\n",
            "Train Epoch: 13 [19200/59872] Loss: 0.008337\n",
            "Train Epoch: 13 [25600/59872] Loss: 0.066941\n",
            "Train Epoch: 13 [32000/59872] Loss: 0.000598\n",
            "Train Epoch: 13 [38400/59872] Loss: 0.091507\n",
            "Train Epoch: 13 [44800/59872] Loss: 0.000111\n",
            "Train Epoch: 13 [51200/59872] Loss: 0.032065\n",
            "Train Epoch: 13 [57600/59872] Loss: 0.037660\n",
            "\n",
            "Test set: Average loss: 0.0001, Accuracy: 9758/10000 (97.58%)\n",
            "\n",
            "Train Epoch: 14 [0/59872] Loss: 0.001939\n",
            "Train Epoch: 14 [6400/59872] Loss: 0.017919\n",
            "Train Epoch: 14 [12800/59872] Loss: 0.003503\n",
            "Train Epoch: 14 [19200/59872] Loss: 0.000940\n",
            "Train Epoch: 14 [25600/59872] Loss: 0.001109\n",
            "Train Epoch: 14 [32000/59872] Loss: 0.000157\n",
            "Train Epoch: 14 [38400/59872] Loss: 0.008161\n",
            "Train Epoch: 14 [44800/59872] Loss: 0.000089\n",
            "Train Epoch: 14 [51200/59872] Loss: 0.064056\n",
            "Train Epoch: 14 [57600/59872] Loss: 0.004964\n",
            "\n",
            "Test set: Average loss: 0.0001, Accuracy: 9798/10000 (97.98%)\n",
            "\n",
            "Train Epoch: 15 [0/59872] Loss: 0.000457\n",
            "Train Epoch: 15 [6400/59872] Loss: 0.012005\n",
            "Train Epoch: 15 [12800/59872] Loss: 0.016301\n",
            "Train Epoch: 15 [19200/59872] Loss: 0.000848\n",
            "Train Epoch: 15 [25600/59872] Loss: 0.000112\n",
            "Train Epoch: 15 [32000/59872] Loss: 0.036559\n",
            "Train Epoch: 15 [38400/59872] Loss: 0.001695\n",
            "Train Epoch: 15 [44800/59872] Loss: 0.000967\n",
            "Train Epoch: 15 [51200/59872] Loss: 0.012221\n",
            "Train Epoch: 15 [57600/59872] Loss: 0.005952\n",
            "\n",
            "Test set: Average loss: 0.0001, Accuracy: 9791/10000 (97.91%)\n",
            "\n",
            "Train Epoch: 16 [0/59872] Loss: 0.003921\n",
            "Train Epoch: 16 [6400/59872] Loss: 0.000175\n",
            "Train Epoch: 16 [12800/59872] Loss: 0.000062\n",
            "Train Epoch: 16 [19200/59872] Loss: 0.000059\n",
            "Train Epoch: 16 [25600/59872] Loss: 0.000006\n",
            "Train Epoch: 16 [32000/59872] Loss: 0.015649\n",
            "Train Epoch: 16 [38400/59872] Loss: 0.000256\n",
            "Train Epoch: 16 [44800/59872] Loss: 0.000453\n",
            "Train Epoch: 16 [51200/59872] Loss: 0.019697\n",
            "Train Epoch: 16 [57600/59872] Loss: 0.002127\n",
            "\n",
            "Test set: Average loss: 0.0001, Accuracy: 9783/10000 (97.83%)\n",
            "\n",
            "Train Epoch: 17 [0/59872] Loss: 0.011835\n",
            "Train Epoch: 17 [6400/59872] Loss: 0.000455\n",
            "Train Epoch: 17 [12800/59872] Loss: 0.000233\n",
            "Train Epoch: 17 [19200/59872] Loss: 0.000024\n",
            "Train Epoch: 17 [25600/59872] Loss: 0.000406\n",
            "Train Epoch: 17 [32000/59872] Loss: 0.098220\n",
            "Train Epoch: 17 [38400/59872] Loss: 0.000027\n",
            "Train Epoch: 17 [44800/59872] Loss: 0.000075\n",
            "Train Epoch: 17 [51200/59872] Loss: 0.000367\n",
            "Train Epoch: 17 [57600/59872] Loss: 0.001513\n",
            "\n",
            "Test set: Average loss: 0.0001, Accuracy: 9798/10000 (97.98%)\n",
            "\n",
            "Train Epoch: 18 [0/59872] Loss: 0.001359\n",
            "Train Epoch: 18 [6400/59872] Loss: 0.004870\n",
            "Train Epoch: 18 [12800/59872] Loss: 0.000330\n",
            "Train Epoch: 18 [19200/59872] Loss: 0.000021\n",
            "Train Epoch: 18 [25600/59872] Loss: 0.001091\n",
            "Train Epoch: 18 [32000/59872] Loss: 0.001503\n",
            "Train Epoch: 18 [38400/59872] Loss: 0.000620\n",
            "Train Epoch: 18 [44800/59872] Loss: 0.000068\n",
            "Train Epoch: 18 [51200/59872] Loss: 0.003351\n",
            "Train Epoch: 18 [57600/59872] Loss: 0.000736\n",
            "\n",
            "Test set: Average loss: 0.0001, Accuracy: 9801/10000 (98.01%)\n",
            "\n",
            "Train Epoch: 19 [0/59872] Loss: 0.005215\n",
            "Train Epoch: 19 [6400/59872] Loss: 0.001658\n",
            "Train Epoch: 19 [12800/59872] Loss: 0.000679\n",
            "Train Epoch: 19 [19200/59872] Loss: 0.000156\n",
            "Train Epoch: 19 [25600/59872] Loss: 0.000809\n",
            "Train Epoch: 19 [32000/59872] Loss: 0.001023\n",
            "Train Epoch: 19 [38400/59872] Loss: 0.003554\n",
            "Train Epoch: 19 [44800/59872] Loss: 0.006733\n",
            "Train Epoch: 19 [51200/59872] Loss: 0.000272\n",
            "Train Epoch: 19 [57600/59872] Loss: 0.001002\n",
            "\n",
            "Test set: Average loss: 0.0001, Accuracy: 9800/10000 (98.00%)\n",
            "\n",
            "Train Epoch: 20 [0/59872] Loss: 0.013033\n",
            "Train Epoch: 20 [6400/59872] Loss: 0.000225\n",
            "Train Epoch: 20 [12800/59872] Loss: 0.000023\n",
            "Train Epoch: 20 [19200/59872] Loss: 0.001000\n",
            "Train Epoch: 20 [25600/59872] Loss: 0.000227\n",
            "Train Epoch: 20 [32000/59872] Loss: 0.039637\n",
            "Train Epoch: 20 [38400/59872] Loss: 0.000787\n",
            "Train Epoch: 20 [44800/59872] Loss: 0.001392\n",
            "Train Epoch: 20 [51200/59872] Loss: 0.002095\n",
            "Train Epoch: 20 [57600/59872] Loss: 0.000003\n",
            "\n",
            "Test set: Average loss: 0.0001, Accuracy: 9793/10000 (97.93%)\n",
            "\n",
            "Train Epoch: 21 [0/59872] Loss: 0.004540\n",
            "Train Epoch: 21 [6400/59872] Loss: 0.000006\n",
            "Train Epoch: 21 [12800/59872] Loss: 0.009069\n",
            "Train Epoch: 21 [19200/59872] Loss: 0.044786\n",
            "Train Epoch: 21 [25600/59872] Loss: 0.000103\n",
            "Train Epoch: 21 [32000/59872] Loss: 0.028963\n",
            "Train Epoch: 21 [38400/59872] Loss: 0.000433\n",
            "Train Epoch: 21 [44800/59872] Loss: 0.001135\n",
            "Train Epoch: 21 [51200/59872] Loss: 0.000004\n",
            "Train Epoch: 21 [57600/59872] Loss: 0.000023\n",
            "\n",
            "Test set: Average loss: 0.0001, Accuracy: 9824/10000 (98.24%)\n",
            "\n",
            "Train Epoch: 22 [0/59872] Loss: 0.000138\n",
            "Train Epoch: 22 [6400/59872] Loss: 0.000000\n",
            "Train Epoch: 22 [12800/59872] Loss: 0.000081\n",
            "Train Epoch: 22 [19200/59872] Loss: 0.000074\n",
            "Train Epoch: 22 [25600/59872] Loss: 0.000411\n",
            "Train Epoch: 22 [32000/59872] Loss: 0.000040\n",
            "Train Epoch: 22 [38400/59872] Loss: 0.000005\n",
            "Train Epoch: 22 [44800/59872] Loss: 0.008760\n",
            "Train Epoch: 22 [51200/59872] Loss: 0.010951\n",
            "Train Epoch: 22 [57600/59872] Loss: 0.001864\n",
            "\n",
            "Test set: Average loss: 0.0001, Accuracy: 9778/10000 (97.78%)\n",
            "\n",
            "Train Epoch: 23 [0/59872] Loss: 0.000091\n",
            "Train Epoch: 23 [6400/59872] Loss: 0.000024\n",
            "Train Epoch: 23 [12800/59872] Loss: 0.000001\n",
            "Train Epoch: 23 [19200/59872] Loss: 0.000003\n",
            "Train Epoch: 23 [25600/59872] Loss: 0.000246\n",
            "Train Epoch: 23 [32000/59872] Loss: 0.013025\n",
            "Train Epoch: 23 [38400/59872] Loss: 0.134151\n",
            "Train Epoch: 23 [44800/59872] Loss: 0.001815\n",
            "Train Epoch: 23 [51200/59872] Loss: 0.067647\n",
            "Train Epoch: 23 [57600/59872] Loss: 0.073951\n",
            "\n",
            "Test set: Average loss: 0.0001, Accuracy: 9803/10000 (98.03%)\n",
            "\n",
            "Train Epoch: 24 [0/59872] Loss: 0.012914\n",
            "Train Epoch: 24 [6400/59872] Loss: 0.035747\n",
            "Train Epoch: 24 [12800/59872] Loss: 0.022987\n",
            "Train Epoch: 24 [19200/59872] Loss: 0.000007\n",
            "Train Epoch: 24 [25600/59872] Loss: 0.002389\n",
            "Train Epoch: 24 [32000/59872] Loss: 0.130100\n",
            "Train Epoch: 24 [38400/59872] Loss: 0.008737\n",
            "Train Epoch: 24 [44800/59872] Loss: 0.000152\n",
            "Train Epoch: 24 [51200/59872] Loss: 0.070032\n",
            "Train Epoch: 24 [57600/59872] Loss: 0.000025\n",
            "\n",
            "Test set: Average loss: 0.0001, Accuracy: 9802/10000 (98.02%)\n",
            "\n",
            "Train Epoch: 25 [0/59872] Loss: 0.044505\n",
            "Train Epoch: 25 [6400/59872] Loss: 0.000054\n",
            "Train Epoch: 25 [12800/59872] Loss: 0.007196\n",
            "Train Epoch: 25 [19200/59872] Loss: 0.022670\n",
            "Train Epoch: 25 [25600/59872] Loss: 0.000132\n",
            "Train Epoch: 25 [32000/59872] Loss: 0.000065\n",
            "Train Epoch: 25 [38400/59872] Loss: 0.000085\n",
            "Train Epoch: 25 [44800/59872] Loss: 0.014312\n",
            "Train Epoch: 25 [51200/59872] Loss: 0.004244\n",
            "Train Epoch: 25 [57600/59872] Loss: 0.066018\n",
            "\n",
            "Test set: Average loss: 0.0001, Accuracy: 9820/10000 (98.20%)\n",
            "\n"
          ]
        }
      ],
      "source": [
        "import torch\n",
        "import torch.nn as nn\n",
        "import torch.optim as optim\n",
        "import torch.nn.functional as F\n",
        "from torchvision import datasets, transforms\n",
        "from torch.utils.data import DataLoader, Subset\n",
        "\n",
        "\n",
        "# Set device\n",
        "from google.colab import drive\n",
        "device = torch.device('cuda' if torch.cuda.is_available() else 'cpu')\n",
        "drive.mount('/content/gdrive')\n",
        "\n",
        "# Save the model files\n",
        "PATH = '/content/gdrive/My Drive/Colab Notebooks/chkpt_training_BN/'\n",
        "No_Epochs = 25\n",
        "# Define the neural network with 3 Linear layers and SiLU activation\n",
        "class MLP(nn.Module):\n",
        "    def __init__(self):\n",
        "        super(MLP, self).__init__()\n",
        "        self.flatten = nn.Flatten()\n",
        "        self.silu = nn.SiLU()\n",
        "        self.fc1 = nn.Linear(28*28, 512 ,bias=False)  # Input layer (flattened 28x28 images), 512 neurons\n",
        "        self.fc2 = nn.Linear(512, 256, bias=False)     # Hidden layer with 256 neurons\n",
        "        self.fc3 = nn.Linear(256, 10, bias=False)      # Output layer (10 classes for digits 0-9)\n",
        "\n",
        "    def forward(self, x):\n",
        "        x = self.flatten(x)  # Flatten the input image\n",
        "        x = self.silu(self.fc1(x))\n",
        "        x = self.silu(self.fc2(x))\n",
        "        x = self.fc3(x)        # Output without activation for classification\n",
        "        return x\n",
        "\n",
        "\n",
        "# Define transformations for the dataset\n",
        "transform = transforms.Compose([\n",
        "    # transforms.RandomRotation(10),\n",
        "    # transforms.RandomAffine(0, translate=(0.1,0.1)),\n",
        "    transforms.ToTensor(),\n",
        "    transforms.Normalize((0.1307,), (0.3081,))  # Normalize with mean and std of MNIST\n",
        "])\n",
        "no_imgs_eval_set = 128\n",
        "mnist_data = datasets.MNIST(root='./data', train=True, transform=transform, download=True)\n",
        "eval_indices = list(range(no_imgs_eval_set))\n",
        "train_indices = list(range(no_imgs_eval_set, len(mnist_data)))\n",
        "\n",
        "# Load the MNIST dataset\n",
        "eval_set = Subset(mnist_data, eval_indices)\n",
        "train_dataset = Subset(mnist_data, train_indices)\n",
        "test_dataset = datasets.MNIST(root='./data', train=False, transform=transform)\n",
        "\n",
        "train_loader = DataLoader(train_dataset, batch_size=64, shuffle=True)\n",
        "test_loader = DataLoader(test_dataset, batch_size=1000, shuffle=False)\n",
        "eval_loader = DataLoader(eval_set, batch_size=64, shuffle=False)\n",
        "\n",
        "# Initialize model, loss function, and optimizer\n",
        "model = MLP().to(device)\n",
        "criterion = nn.CrossEntropyLoss()\n",
        "optimizer = optim.Adam(model.parameters(), lr=0.001)\n",
        "\n",
        "# Training loop\n",
        "def train(model, device, train_loader, optimizer, criterion, epoch):\n",
        "    model.train()\n",
        "    for batch_idx, (data, target) in enumerate(train_loader):\n",
        "        data, target = data.to(device), target.to(device)\n",
        "        optimizer.zero_grad()\n",
        "        output = model(data)\n",
        "        loss = criterion(output, target)\n",
        "        loss.backward()\n",
        "        optimizer.step()\n",
        "        if batch_idx % 100 == 0:\n",
        "            print(f'Train Epoch: {epoch} [{batch_idx*len(data)}/{len(train_loader.dataset)}] Loss: {loss.item():.6f}')\n",
        "\n",
        "# Test function\n",
        "def test(model, device, test_loader):\n",
        "    model.eval()\n",
        "    test_loss = 0\n",
        "    correct = 0\n",
        "    with torch.no_grad():\n",
        "        for data, target in test_loader:\n",
        "            data, target = data.to(device), target.to(device)\n",
        "            output = model(data)\n",
        "            test_loss += criterion(output, target).item()  # Sum up batch loss\n",
        "            pred = output.argmax(dim=1, keepdim=True)  # Get the index of the max log-probability\n",
        "            correct += pred.eq(target.view_as(pred)).sum().item()\n",
        "\n",
        "    test_loss /= len(test_loader.dataset)\n",
        "    accuracy = 100. * correct / len(test_loader.dataset)\n",
        "    print(f'\\nTest set: Average loss: {test_loss:.4f}, Accuracy: {correct}/{len(test_loader.dataset)} ({accuracy:.2f}%)\\n')\n",
        "\n",
        "# Save checkpoint function\n",
        "def save_checkpoint(epoch, model, optimizer, path):\n",
        "  torch.save({\n",
        "      'epoch': epoch,\n",
        "      'model_state_dict': model.state_dict(),\n",
        "      'optimizer_state_dict': optimizer.state_dict()\n",
        "  }, path)\n",
        "\n",
        "# Main training and testing loop\n",
        "\n",
        "for epoch in range(1, No_Epochs+1):  # Train for 5 epochs\n",
        "    train(model, device, train_loader, optimizer, criterion, epoch)\n",
        "    test(model, device, test_loader)\n",
        "    checkpoint_path= PATH + f'checkpoint_epoch_{epoch}.pth'\n",
        "    save_checkpoint(epoch, model, optimizer, checkpoint_path)"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "#Util functions\n",
        "import os\n",
        "import tqdm\n",
        "def print_size_of_model(model):\n",
        "    torch.save(model.state_dict(), \"temp_delme.p\")\n",
        "    print('Size (KB):', os.path.getsize(\"temp_delme.p\")/1e3)\n",
        "    os.remove('temp_delme.p')\n",
        "    # return True\n",
        "# Model analysis Current Weights and size of the model before Quantization\n",
        "def Model_analysis(model):\n",
        "  print('Weights Before Quantization')\n",
        "  print(model.fc1.weight)\n",
        "  print(\"\\nData Type of Model:\",model.fc1.weight.dtype)\n",
        "  print('Size of the model before Quantization')\n",
        "  print_size_of_model(model)\n",
        "  test(model, device,eval_loader)\n",
        "  # return True\n",
        "\n"
      ],
      "metadata": {
        "id": "a2lhWM5w7YVc"
      },
      "execution_count": 15,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# Load Model\n",
        "Model_analysis(model)"
      ],
      "metadata": {
        "id": "7M6sgQxFYKMT",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "f0554c60-8f13-421d-ec84-209179aa9b3a"
      },
      "execution_count": 16,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Weights Before Quantization\n",
            "Parameter containing:\n",
            "tensor([[0.0088, 0.0624, 0.0083,  ..., 0.0267, 0.0469, 0.0489],\n",
            "        [0.1012, 0.1306, 0.1138,  ..., 0.1344, 0.1305, 0.1118],\n",
            "        [0.0758, 0.0833, 0.0674,  ..., 0.1113, 0.0828, 0.0540],\n",
            "        ...,\n",
            "        [0.1134, 0.1597, 0.1541,  ..., 0.1217, 0.1074, 0.1076],\n",
            "        [0.0843, 0.0569, 0.1004,  ..., 0.1168, 0.1112, 0.0988],\n",
            "        [0.1139, 0.0734, 0.0982,  ..., 0.0646, 0.0788, 0.0551]],\n",
            "       device='cuda:0', requires_grad=True)\n",
            "\n",
            "Data Type of Model: torch.float32\n",
            "Size of the model before Quantization\n",
            "Size (KB): 2142.121\n",
            "\n",
            "Test set: Average loss: 0.0028, Accuracy: 126/128 (98.44%)\n",
            "\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "Quantization Techniques\n",
        "1. Post Training Quantization (PTQ)\n",
        "2. Quantize Aware Training (QAT)"
      ],
      "metadata": {
        "id": "ru90EhQf6Zq-"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "# Post Training Quantization Method\n",
        "Steps\n",
        "1. Insert Min-Max based oberservers\n",
        "2. Calibirate\n",
        "3. Quantize the model\n",
        "4. Compare the results with original vs Quantized model\n"
      ],
      "metadata": {
        "id": "3ojrlH2IBAD4"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "class Quantized_MLP(nn.Module):\n",
        "  def __init__(self):\n",
        "    super(Quantized_MLP, self).__init__()\n",
        "    self.quant = torch.quantization.QuantStub()\n",
        "    self.flatten = nn.Flatten()\n",
        "    self.silu = nn.SiLU()\n",
        "    self.fc1 = nn.Linear(28*28, 512 ,bias=False)  # Input layer (flattened 28x28 images), 512 neurons\n",
        "    self.fc2 = nn.Linear(512, 256, bias=False)     # Hidden layer with 256 neurons\n",
        "    self.fc3 = nn.Linear(256, 10, bias=False)      # Output layer (10 classes for digits 0-9)\n",
        "    self.dequant = torch.quantization.DeQuantStub()\n",
        "\n",
        "\n",
        "  def forward(self, x):\n",
        "    x = self.flatten(x)  # Flatten the input image\n",
        "    x = self.quant(x)\n",
        "    x = self.silu(self.fc1(x))\n",
        "    x = self.silu(self.fc2(x))\n",
        "    x = self.fc3(x)        # Output without activation for classification\n",
        "    x = self.dequant(x)\n",
        "    return x"
      ],
      "metadata": {
        "id": "qCEC7v3A8wVA"
      },
      "execution_count": 17,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "quantized_model = Quantized_MLP().to(device)\n",
        "quantized_model.load_state_dict(model.state_dict())\n",
        "quantized_model.eval()\n",
        "\n",
        "quantized_model.qconfig = torch.ao.quantization.default_qconfig\n",
        "quantized_model = torch.ao.quantization.prepare(quantized_model)\n",
        "quantized_model"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "0g6EbctXJCAY",
        "outputId": "1a04a0aa-145b-4e3e-9ee3-908fb022b1df"
      },
      "execution_count": 26,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "Quantized_MLP(\n",
              "  (quant): QuantStub(\n",
              "    (activation_post_process): MinMaxObserver(min_val=inf, max_val=-inf)\n",
              "  )\n",
              "  (flatten): Flatten(start_dim=1, end_dim=-1)\n",
              "  (silu): SiLU()\n",
              "  (fc1): Linear(\n",
              "    in_features=784, out_features=512, bias=False\n",
              "    (activation_post_process): MinMaxObserver(min_val=inf, max_val=-inf)\n",
              "  )\n",
              "  (fc2): Linear(\n",
              "    in_features=512, out_features=256, bias=False\n",
              "    (activation_post_process): MinMaxObserver(min_val=inf, max_val=-inf)\n",
              "  )\n",
              "  (fc3): Linear(\n",
              "    in_features=256, out_features=10, bias=False\n",
              "    (activation_post_process): MinMaxObserver(min_val=inf, max_val=-inf)\n",
              "  )\n",
              "  (dequant): DeQuantStub()\n",
              ")"
            ]
          },
          "metadata": {},
          "execution_count": 26
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "Step 2: Calibrate the model using test set"
      ],
      "metadata": {
        "id": "fVwMGIqyDVs2"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "test(quantized_model, device, eval_loader) # Calibered on the evalset saved"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "X-orpCPDDSdw",
        "outputId": "4293401f-cdf8-4f7d-97c6-3d8b80d4be0c"
      },
      "execution_count": 27,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "\n",
            "Test set: Average loss: 0.0028, Accuracy: 126/128 (98.44%)\n",
            "\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "print('Check the Quantized model details\\n', quantized_model)"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "00aidTMlDwhv",
        "outputId": "059c2d84-6d05-4e84-d545-c0bac8493269"
      },
      "execution_count": 28,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Check the Quantized model details\n",
            " Quantized_MLP(\n",
            "  (quant): QuantStub(\n",
            "    (activation_post_process): MinMaxObserver(min_val=-0.4242129623889923, max_val=2.821486711502075)\n",
            "  )\n",
            "  (flatten): Flatten(start_dim=1, end_dim=-1)\n",
            "  (silu): SiLU()\n",
            "  (fc1): Linear(\n",
            "    in_features=784, out_features=512, bias=False\n",
            "    (activation_post_process): MinMaxObserver(min_val=-60.48722839355469, max_val=36.19487380981445)\n",
            "  )\n",
            "  (fc2): Linear(\n",
            "    in_features=512, out_features=256, bias=False\n",
            "    (activation_post_process): MinMaxObserver(min_val=-88.3974609375, max_val=62.31698226928711)\n",
            "  )\n",
            "  (fc3): Linear(\n",
            "    in_features=256, out_features=10, bias=False\n",
            "    (activation_post_process): MinMaxObserver(min_val=-112.92228698730469, max_val=69.16704559326172)\n",
            "  )\n",
            "  (dequant): DeQuantStub()\n",
            ")\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "Step 3: Quantize the model to pytorch default int8"
      ],
      "metadata": {
        "id": "2dAn1lUAEXXf"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "quantized_model_converted = torch.ao.quantization.convert(quantized_model)\n",
        "print('check statistics of the quantized model\\n', quantized_model_converted)\n",
        "# Check the weights of the quantized model\n",
        "print('Check the weights of the quantized model\\n',torch.int_repr(quantized_model_converted.fc1.weight()))"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "Kw1djZYgD9EA",
        "outputId": "626946e7-7540-4d09-ec94-0128ef8efe81"
      },
      "execution_count": 29,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "check statistics of the quantized model\n",
            " Quantized_MLP(\n",
            "  (quant): Quantize(scale=tensor([0.0256], device='cuda:0'), zero_point=tensor([17], device='cuda:0'), dtype=torch.quint8)\n",
            "  (flatten): Flatten(start_dim=1, end_dim=-1)\n",
            "  (silu): SiLU()\n",
            "  (fc1): QuantizedLinear(in_features=784, out_features=512, scale=0.761276364326477, zero_point=79, qscheme=torch.per_tensor_affine)\n",
            "  (fc2): QuantizedLinear(in_features=512, out_features=256, scale=1.1867278814315796, zero_point=74, qscheme=torch.per_tensor_affine)\n",
            "  (fc3): QuantizedLinear(in_features=256, out_features=10, scale=1.4337742328643799, zero_point=79, qscheme=torch.per_tensor_affine)\n",
            "  (dequant): DeQuantize()\n",
            ")\n",
            "Check the weights of the quantized model\n",
            " tensor([[ 1,  7,  1,  ...,  3,  6,  6],\n",
            "        [12, 16, 14,  ..., 16, 16, 13],\n",
            "        [ 9, 10,  8,  ..., 13, 10,  6],\n",
            "        ...,\n",
            "        [14, 19, 18,  ..., 15, 13, 13],\n",
            "        [10,  7, 12,  ..., 14, 13, 12],\n",
            "        [14,  9, 12,  ...,  8,  9,  7]], device='cuda:0', dtype=torch.int8)\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "Step4: Compare the results of PTQ int8 model vs original fp32 model"
      ],
      "metadata": {
        "id": "7_-qG5U6J4OG"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "print('Original weights: ')\n",
        "print(model.fc1.weight)\n",
        "print('')\n",
        "print(f'Dequantized weights: ')\n",
        "print(torch.dequantize(quantized_model_converted.fc1.weight()))\n",
        "print('')\n",
        "\n",
        "print(\"Check the model after quantization\")\n",
        "print_size_of_model(quantized_model_converted)\n"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "rG1TnK7qIEGh",
        "outputId": "61219a45-4570-4242-83ad-1c62844acf0f"
      },
      "execution_count": 30,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Original weights: \n",
            "Parameter containing:\n",
            "tensor([[0.0088, 0.0624, 0.0083,  ..., 0.0267, 0.0469, 0.0489],\n",
            "        [0.1012, 0.1306, 0.1138,  ..., 0.1344, 0.1305, 0.1118],\n",
            "        [0.0758, 0.0833, 0.0674,  ..., 0.1113, 0.0828, 0.0540],\n",
            "        ...,\n",
            "        [0.1134, 0.1597, 0.1541,  ..., 0.1217, 0.1074, 0.1076],\n",
            "        [0.0843, 0.0569, 0.1004,  ..., 0.1168, 0.1112, 0.0988],\n",
            "        [0.1139, 0.0734, 0.0982,  ..., 0.0646, 0.0788, 0.0551]],\n",
            "       device='cuda:0', requires_grad=True)\n",
            "\n",
            "Dequantized weights: \n",
            "tensor([[0.0084, 0.0587, 0.0084,  ..., 0.0252, 0.0503, 0.0503],\n",
            "        [0.1006, 0.1342, 0.1174,  ..., 0.1342, 0.1342, 0.1090],\n",
            "        [0.0755, 0.0839, 0.0671,  ..., 0.1090, 0.0839, 0.0503],\n",
            "        ...,\n",
            "        [0.1174, 0.1593, 0.1509,  ..., 0.1258, 0.1090, 0.1090],\n",
            "        [0.0839, 0.0587, 0.1006,  ..., 0.1174, 0.1090, 0.1006],\n",
            "        [0.1174, 0.0755, 0.1006,  ..., 0.0671, 0.0755, 0.0587]],\n",
            "       device='cuda:0')\n",
            "\n",
            "Check the model after quantization\n",
            "Size (KB): 539.49\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "print(\"Original Model accuracy\")\n",
        "test(model,device, eval_loader)\n",
        "print('Quantized model accuracy')\n",
        "test(quantized_model, device, eval_loader)"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "wNIB_IFBLxZV",
        "outputId": "ffe2a132-f121-459a-a4cf-08aebcf9aa96"
      },
      "execution_count": 42,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Original Model accuracy\n",
            "\n",
            "Test set: Average loss: 0.0028, Accuracy: 126/128 (98.44%)\n",
            "\n",
            "Quantized model accuracy\n",
            "\n",
            "Test set: Average loss: 0.0028, Accuracy: 126/128 (98.44%)\n",
            "\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "2. Quantized Aware Training"
      ],
      "metadata": {
        "id": "kyo5Il2IIGHC"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "class Quantized_MLP(nn.Module):\n",
        "  def __init__(self):\n",
        "    super(Quantized_MLP, self).__init__()\n",
        "    self.quant = torch.quantization.QuantStub()\n",
        "    self.flatten = nn.Flatten()\n",
        "    self.silu = nn.SiLU()\n",
        "    self.fc1 = nn.Linear(28*28, 512 ,bias=False)  # Input layer (flattened 28x28 images), 512 neurons\n",
        "    self.fc2 = nn.Linear(512, 256, bias=False)     # Hidden layer with 256 neurons\n",
        "    self.fc3 = nn.Linear(256, 10, bias=False)      # Output layer (10 classes for digits 0-9)\n",
        "    self.dequant = torch.quantization.DeQuantStub()\n",
        "\n",
        "\n",
        "  def forward(self, x):\n",
        "    x = self.quant(x)\n",
        "    x = self.flatten(x)  # Flatten the input image\n",
        "    x = self.silu(self.fc1(x))\n",
        "    x = self.silu(self.fc2(x))\n",
        "    x = self.fc3(x)        # Output without activation for classification\n",
        "    x = self.dequant(x)\n",
        "    return x\n",
        "QAT_model = Quantized_MLP().to(device)"
      ],
      "metadata": {
        "id": "xdVS2NETIRNL"
      },
      "execution_count": 46,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "QAT_model.qconfig = torch.ao.quantization.default_qconfig\n",
        "QAT_model.train()\n",
        "QAT_model_quantized = torch.ao.quantization.prepare_qat(QAT_model) # Insert observers\n",
        "QAT_model_quantized"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "TNfwhWU8QwRV",
        "outputId": "21c10a05-b3bd-4225-bc0b-70f8b9c920f1"
      },
      "execution_count": 47,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "Quantized_MLP(\n",
              "  (quant): QuantStub(\n",
              "    (activation_post_process): MinMaxObserver(min_val=inf, max_val=-inf)\n",
              "  )\n",
              "  (flatten): Flatten(start_dim=1, end_dim=-1)\n",
              "  (silu): SiLU()\n",
              "  (fc1): Linear(\n",
              "    in_features=784, out_features=512, bias=False\n",
              "    (weight_fake_quant): MinMaxObserver(min_val=inf, max_val=-inf)\n",
              "    (activation_post_process): MinMaxObserver(min_val=inf, max_val=-inf)\n",
              "  )\n",
              "  (fc2): Linear(\n",
              "    in_features=512, out_features=256, bias=False\n",
              "    (weight_fake_quant): MinMaxObserver(min_val=inf, max_val=-inf)\n",
              "    (activation_post_process): MinMaxObserver(min_val=inf, max_val=-inf)\n",
              "  )\n",
              "  (fc3): Linear(\n",
              "    in_features=256, out_features=10, bias=False\n",
              "    (weight_fake_quant): MinMaxObserver(min_val=inf, max_val=-inf)\n",
              "    (activation_post_process): MinMaxObserver(min_val=inf, max_val=-inf)\n",
              "  )\n",
              "  (dequant): DeQuantStub()\n",
              ")"
            ]
          },
          "metadata": {},
          "execution_count": 47
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "PATH_Quant = '/content/gdrive/MyDrive/Colab Notebooks/chkpt_training_quant/'\n",
        "No_Epochs=6\n",
        "for epoch in range(1, No_Epochs+1):  # Train for 5 epochs\n",
        "    train(QAT_model_quantized, device, train_loader, optimizer, criterion, epoch)\n",
        "    test(QAT_model_quantized, device, test_loader)\n",
        "    checkpoint_path= PATH + f'checkpoint_epoch_{epoch}.pth'\n",
        "    save_checkpoint(epoch, QAT_model_quantized, optimizer, checkpoint_path)"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "cz5OMEz_REL5",
        "outputId": "a68076d5-90c6-4bb8-895a-ac1116907add"
      },
      "execution_count": 48,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Train Epoch: 1 [0/59872] Loss: 2.301084\n",
            "Train Epoch: 1 [6400/59872] Loss: 2.291529\n",
            "Train Epoch: 1 [12800/59872] Loss: 2.287277\n",
            "Train Epoch: 1 [19200/59872] Loss: 2.305913\n",
            "Train Epoch: 1 [25600/59872] Loss: 2.301478\n",
            "Train Epoch: 1 [32000/59872] Loss: 2.293453\n",
            "Train Epoch: 1 [38400/59872] Loss: 2.284954\n",
            "Train Epoch: 1 [44800/59872] Loss: 2.294887\n",
            "Train Epoch: 1 [51200/59872] Loss: 2.302283\n",
            "Train Epoch: 1 [57600/59872] Loss: 2.300330\n",
            "\n",
            "Test set: Average loss: 0.0023, Accuracy: 1256/10000 (12.56%)\n",
            "\n",
            "Train Epoch: 2 [0/59872] Loss: 2.300718\n",
            "Train Epoch: 2 [6400/59872] Loss: 2.302331\n",
            "Train Epoch: 2 [12800/59872] Loss: 2.293987\n",
            "Train Epoch: 2 [19200/59872] Loss: 2.306480\n",
            "Train Epoch: 2 [25600/59872] Loss: 2.298894\n",
            "Train Epoch: 2 [32000/59872] Loss: 2.293580\n",
            "Train Epoch: 2 [38400/59872] Loss: 2.302183\n",
            "Train Epoch: 2 [44800/59872] Loss: 2.311525\n",
            "Train Epoch: 2 [51200/59872] Loss: 2.298345\n",
            "Train Epoch: 2 [57600/59872] Loss: 2.297176\n",
            "\n",
            "Test set: Average loss: 0.0023, Accuracy: 1256/10000 (12.56%)\n",
            "\n",
            "Train Epoch: 3 [0/59872] Loss: 2.306721\n",
            "Train Epoch: 3 [6400/59872] Loss: 2.297245\n",
            "Train Epoch: 3 [12800/59872] Loss: 2.297929\n",
            "Train Epoch: 3 [19200/59872] Loss: 2.295868\n",
            "Train Epoch: 3 [25600/59872] Loss: 2.295148\n",
            "Train Epoch: 3 [32000/59872] Loss: 2.305292\n",
            "Train Epoch: 3 [38400/59872] Loss: 2.305006\n",
            "Train Epoch: 3 [44800/59872] Loss: 2.300610\n",
            "Train Epoch: 3 [51200/59872] Loss: 2.295839\n",
            "Train Epoch: 3 [57600/59872] Loss: 2.305717\n",
            "\n",
            "Test set: Average loss: 0.0023, Accuracy: 1256/10000 (12.56%)\n",
            "\n",
            "Train Epoch: 4 [0/59872] Loss: 2.307342\n",
            "Train Epoch: 4 [6400/59872] Loss: 2.295113\n",
            "Train Epoch: 4 [12800/59872] Loss: 2.301341\n",
            "Train Epoch: 4 [19200/59872] Loss: 2.298059\n",
            "Train Epoch: 4 [25600/59872] Loss: 2.294730\n",
            "Train Epoch: 4 [32000/59872] Loss: 2.299874\n",
            "Train Epoch: 4 [38400/59872] Loss: 2.295649\n",
            "Train Epoch: 4 [44800/59872] Loss: 2.297072\n",
            "Train Epoch: 4 [51200/59872] Loss: 2.301214\n",
            "Train Epoch: 4 [57600/59872] Loss: 2.303140\n",
            "\n",
            "Test set: Average loss: 0.0023, Accuracy: 1256/10000 (12.56%)\n",
            "\n",
            "Train Epoch: 5 [0/59872] Loss: 2.291667\n",
            "Train Epoch: 5 [6400/59872] Loss: 2.294851\n",
            "Train Epoch: 5 [12800/59872] Loss: 2.303099\n",
            "Train Epoch: 5 [19200/59872] Loss: 2.302866\n",
            "Train Epoch: 5 [25600/59872] Loss: 2.301751\n",
            "Train Epoch: 5 [32000/59872] Loss: 2.289195\n",
            "Train Epoch: 5 [38400/59872] Loss: 2.301988\n",
            "Train Epoch: 5 [44800/59872] Loss: 2.294106\n",
            "Train Epoch: 5 [51200/59872] Loss: 2.306879\n",
            "Train Epoch: 5 [57600/59872] Loss: 2.300463\n",
            "\n",
            "Test set: Average loss: 0.0023, Accuracy: 1256/10000 (12.56%)\n",
            "\n",
            "Train Epoch: 6 [0/59872] Loss: 2.309586\n",
            "Train Epoch: 6 [6400/59872] Loss: 2.298575\n",
            "Train Epoch: 6 [12800/59872] Loss: 2.300672\n",
            "Train Epoch: 6 [19200/59872] Loss: 2.301850\n",
            "Train Epoch: 6 [25600/59872] Loss: 2.296801\n",
            "Train Epoch: 6 [32000/59872] Loss: 2.292631\n",
            "Train Epoch: 6 [38400/59872] Loss: 2.292942\n",
            "Train Epoch: 6 [44800/59872] Loss: 2.312787\n",
            "Train Epoch: 6 [51200/59872] Loss: 2.298876\n",
            "Train Epoch: 6 [57600/59872] Loss: 2.305480\n",
            "\n",
            "Test set: Average loss: 0.0023, Accuracy: 1256/10000 (12.56%)\n",
            "\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [],
      "metadata": {
        "id": "nzvcwJV66YOU"
      }
    }
  ]
}